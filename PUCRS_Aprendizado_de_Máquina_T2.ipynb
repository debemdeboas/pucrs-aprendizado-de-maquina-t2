{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP3Vmx7ByfowXDqm2dP5L+M",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/debemdeboas/pucrs-aprendizado-de-maquina-t2/blob/master/PUCRS_Aprendizado_de_M%C3%A1quina_T2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download and extract our dataset.\n",
        "\n",
        "This will download a tarred file and extract it into `dist`. Then, we're renaming it to `animes/`.\n",
        "This directory contains the following files:\n",
        "- `animes.csv`, the CSV containing anime IDs, URLs, titles, genres, and poster path\n",
        "- `animes.pkl`, serialized (pickled) list of `Anime` instances. This isn't used by this notebook\n",
        "- `images/`, a directory that contains all of our anime posters as `images/<mal_id>.jpg` files\n"
      ],
      "metadata": {
        "id": "-Sz2tJ-38Ypr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "b9ZTAtF86a_E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4bc22d76-a557-4c61-804b-3e48d30dc7d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mv: cannot stat 'dist': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "\n",
        "ds = requests.get(\"https://public-s3.debem.dev/anime_dataset.tar.xz\", allow_redirects=True)\n",
        "\n",
        "with open(\"anime_dataset.tar.xz\", \"wb\") as f:\n",
        "    f.write(ds.content)\n",
        "\n",
        "!tar xf anime_dataset.tar.xz"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the dataset into memory"
      ],
      "metadata": {
        "id": "q2pATY9W8p9O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"animes.csv\")"
      ],
      "metadata": {
        "id": "zUsy9o3p8sY9"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.dropna()\n",
        "df[\"genres\"] = df[\"genres\"].str.split('|')"
      ],
      "metadata": {
        "id": "zZ8ripRYADsZ"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_genres_to_idx = dict()\n",
        "all_genres = list()\n",
        "for gl in df.genres:\n",
        "    for g in gl:\n",
        "        if g not in all_genres_to_idx:\n",
        "            all_genres.append(g)\n",
        "            all_genres_to_idx[g] = len(all_genres) - 1"
      ],
      "metadata": {
        "id": "yLBqs5DFEC1e"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data.dataset import Dataset\n",
        "from PIL import Image\n",
        "\n",
        "class PosterMultiLabelDataset(Dataset):\n",
        "    def __init__(self, df: pd.DataFrame, *args, **kwargs):\n",
        "        self.df = df\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.loc[idx]\n",
        "        img = Image.open(row[\"img_path\"])\n",
        "\n",
        "        return {\n",
        "            \"image\": img,\n",
        "            # \"genres\": {g: g in row.genres for g in all_genres}\n",
        "            \"genres\": torch.Tensor([1 if g in df.loc[0].genres else 0 for g in all_genres])\n",
        "        }"
      ],
      "metadata": {
        "id": "F9X1yBFZslO5"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's get some transfer learning done.\n",
        "\n",
        "We'll use a pre-trained convolutional network to analyze the posters to define which genres a given anime belongs to.\n",
        "Each anime can belong to any number of genres.\n"
      ],
      "metadata": {
        "id": "0Z5QkRZL8enF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "import torch.nn as nn\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import torchvision\n",
        "import random\n",
        "import torch\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def validation(model, loader, criterion):\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for data in loader:\n",
        "            images, labels = data\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs,labels)\n",
        "            val_loss +=loss\n",
        "    return val_loss/len(loader)\n",
        "\n",
        "def train(model, trainloader, testloader, optimizer, criterion, epochs):\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0\n",
        "        for data in tqdm(trainloader):\n",
        "            images, labels = data\n",
        "            model.zero_grad()\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "        val_loss = validation(model, testloader, criterion)\n",
        "        print(f'Epoch: {epoch+1} | Loss: {running_loss/len(trainloader)} | Val Loss: {val_loss}')\n",
        "\n",
        "def accuracy(model, loader):\n",
        "    model.eval()\n",
        "    corrected = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in loader:\n",
        "            images, labels = data\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _,predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            corrected += (predicted == labels).sum().item()\n",
        "    return corrected * 100 // total\n",
        "\n",
        "def confusion_matrix(model, loader):\n",
        "    model.eval()\n",
        "    confusion_matrix = np.zeros((2,2))\n",
        "    with torch.no_grad():\n",
        "        for data in loader:\n",
        "            images, labels = data\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _,predicted = torch.max(outputs, 1)\n",
        "            for i in range(labels.size(0)):\n",
        "                confusion_matrix[labels[i].item()][predicted[i].item()] += 1\n",
        "    ax = sns.heatmap(confusion_matrix, annot=True, cmap='Blues', fmt='g')\n",
        "    ax.set_xlabel('Predicted')\n",
        "    ax.set_ylabel('Label')\n",
        "    return ax"
      ],
      "metadata": {
        "id": "kXodITD1GmMa"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def imshow(img):\n",
        "    plt.figure(figsize=(20,8))\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "img_size = (256,256)\n",
        "transformations = transforms.Compose([transforms.Resize(img_size)])\n"
      ],
      "metadata": {
        "id": "H9GPqLKEo0xn"
      },
      "execution_count": 65,
      "outputs": []
    }
  ]
}